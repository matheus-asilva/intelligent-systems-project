{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "incomplete-league",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import nltk\n",
    "nltk.download('rslp')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, learning_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator\n",
    "from itertools import combinations\n",
    "import cloudpickle as cp\n",
    "import scikitplot as skplt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-vehicle",
   "metadata": {},
   "source": [
    "# Index:\n",
    "* [Data Extraction](#data-extraction)\n",
    "* [Data Formatting](#data-formatting)\n",
    "    * [NLP](#nlp)\n",
    "    * [Pre-Processor](#pre-processor)\n",
    "    * [Data Split](#data-split)\n",
    "* [Modeling](#modeling)\n",
    "* [Model Validation](#model-validation)\n",
    "    * [Random Search](#random-search)\n",
    "        * [Text Hyperparameters](#text-hyperparameters)\n",
    "        * [Validation Metrics](#validation-metrics)\n",
    "    * [Test Metrics](#test-metrics)\n",
    "* [Model Exportation](#model-exportation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "creative-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-occurrence",
   "metadata": {},
   "source": [
    "## Data Extraction <a class=\"anchor\" id=\"data-extraction\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "driven-scene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38000, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH = os.getenv(\"DATASET_PATH\")\n",
    "\n",
    "sample = pd.read_csv(DATASET_PATH)\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "conventional-stockholm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>query</th>\n",
       "      <th>search_page</th>\n",
       "      <th>position</th>\n",
       "      <th>title</th>\n",
       "      <th>concatenated_tags</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>price</th>\n",
       "      <th>weight</th>\n",
       "      <th>express_delivery</th>\n",
       "      <th>minimum_quantity</th>\n",
       "      <th>view_counts</th>\n",
       "      <th>order_counts</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11394449</td>\n",
       "      <td>8324141</td>\n",
       "      <td>espirito santo</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>Mandala Espírito Santo</td>\n",
       "      <td>mandala mdf</td>\n",
       "      <td>2015-11-14 19:42:12</td>\n",
       "      <td>171.890000</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Decoração</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15534262</td>\n",
       "      <td>6939286</td>\n",
       "      <td>cartao de visita</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Cartão de Visita</td>\n",
       "      <td>cartao visita panfletos tag adesivos copos lon...</td>\n",
       "      <td>2018-04-04 20:55:07</td>\n",
       "      <td>77.670000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Papel e Cia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16153119</td>\n",
       "      <td>9835835</td>\n",
       "      <td>expositor de esmaltes</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>Organizador expositor p/ 70 esmaltes</td>\n",
       "      <td>expositor</td>\n",
       "      <td>2018-10-13 20:57:07</td>\n",
       "      <td>73.920006</td>\n",
       "      <td>2709.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Outros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15877252</td>\n",
       "      <td>8071206</td>\n",
       "      <td>medidas lencol para berco americano</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Jogo de Lençol Berço Estampado</td>\n",
       "      <td>t jogo lencol menino lencol berco</td>\n",
       "      <td>2017-02-27 13:26:03</td>\n",
       "      <td>118.770004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bebê</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15917108</td>\n",
       "      <td>7200773</td>\n",
       "      <td>adesivo box banheiro</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>ADESIVO BOX DE BANHEIRO</td>\n",
       "      <td>adesivo box banheiro</td>\n",
       "      <td>2017-05-09 13:18:38</td>\n",
       "      <td>191.810000</td>\n",
       "      <td>507.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Decoração</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  seller_id                                query  search_page  \\\n",
       "0    11394449    8324141                       espirito santo            2   \n",
       "1    15534262    6939286                     cartao de visita            2   \n",
       "2    16153119    9835835                expositor de esmaltes            1   \n",
       "3    15877252    8071206  medidas lencol para berco americano            1   \n",
       "4    15917108    7200773                 adesivo box banheiro            3   \n",
       "\n",
       "   position                                 title  \\\n",
       "0         6                Mandala Espírito Santo   \n",
       "1         0                      Cartão de Visita   \n",
       "2        38  Organizador expositor p/ 70 esmaltes   \n",
       "3         6        Jogo de Lençol Berço Estampado   \n",
       "4        38               ADESIVO BOX DE BANHEIRO   \n",
       "\n",
       "                                   concatenated_tags        creation_date  \\\n",
       "0                                        mandala mdf  2015-11-14 19:42:12   \n",
       "1  cartao visita panfletos tag adesivos copos lon...  2018-04-04 20:55:07   \n",
       "2                                          expositor  2018-10-13 20:57:07   \n",
       "3                  t jogo lencol menino lencol berco  2017-02-27 13:26:03   \n",
       "4                               adesivo box banheiro  2017-05-09 13:18:38   \n",
       "\n",
       "        price  weight  express_delivery  minimum_quantity  view_counts  \\\n",
       "0  171.890000  1200.0                 1                 4          244   \n",
       "1   77.670000     8.0                 1                 5          124   \n",
       "2   73.920006  2709.0                 1                 1           59   \n",
       "3  118.770004     0.0                 1                 1          180   \n",
       "4  191.810000   507.0                 1                 6           34   \n",
       "\n",
       "   order_counts     category  \n",
       "0           NaN    Decoração  \n",
       "1           NaN  Papel e Cia  \n",
       "2           NaN       Outros  \n",
       "3           1.0         Bebê  \n",
       "4           NaN    Decoração  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "healthy-memphis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id               0\n",
       "seller_id                0\n",
       "query                    0\n",
       "search_page              0\n",
       "position                 0\n",
       "title                    0\n",
       "concatenated_tags        2\n",
       "creation_date            0\n",
       "price                    0\n",
       "weight                  58\n",
       "express_delivery         0\n",
       "minimum_quantity         0\n",
       "view_counts              0\n",
       "order_counts         20105\n",
       "category                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "center-jason",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lembrancinhas         17524\n",
       "Decoração              8723\n",
       "Bebê                   6930\n",
       "Papel e Cia            2750\n",
       "Outros                 1133\n",
       "Bijuterias e Jóias      940\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-recording",
   "metadata": {},
   "source": [
    "## Data Formatting <a class=\"anchor\" id=\"data-formatting\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-softball",
   "metadata": {},
   "source": [
    "We selected all columns from the CSV file, with the exception of the product_id and seller_id columns, as they don't contain meaningful information about the categories we want to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "capital-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = sample.select_dtypes(\"number\").columns.tolist()[2:]\n",
    "text_columns = ['title', 'query', 'concatenated_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "assured-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample[numerical_columns + text_columns]\n",
    "y = sample[\"category\"]\n",
    "\n",
    "X.loc[:, 'title'] = X['title'].str.lower()\n",
    "X.loc[:, text_columns] = X.loc[:, text_columns].fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-ordering",
   "metadata": {},
   "source": [
    "### NLP <a class=\"anchor\" id=\"nlp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-corporation",
   "metadata": {},
   "source": [
    "We used Bag of Words in our pipeline, because it is a simple way to extract information from the text. We also experimented with Tf-idf, which improved our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "white-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextColumnConcatenation(BaseEstimator):\n",
    "\n",
    "    def __init__(self, columns = ['title']):\n",
    "        self.columns = columns\n",
    "        \n",
    "    def transform(self, X):\n",
    "        if len(self.columns) > 1:\n",
    "            return X[self.columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "        else:\n",
    "            return X[self.columns[0]]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "offshore-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regex_word(min_word_length, max_word_length):\n",
    "    return rf'(?u)\\b[^\\d\\W]{{{min_word_length},{max_word_length}}}\\b'\n",
    "\n",
    "class CountVectorizerWithStemming(CountVectorizer):\n",
    "    \n",
    "    def __init__(self, stemming=False, min_word_length=1, max_word_length=20, stop_words=None):\n",
    "        \n",
    "        self.stemming = stemming\n",
    "        self.min_word_length = min_word_length\n",
    "        self.max_word_length = max_word_length\n",
    "        self.stop_words = stop_words\n",
    "    \n",
    "        token_pattern = get_regex_word(min_word_length, max_word_length)\n",
    "        \n",
    "        parameters = {\"token_pattern\": token_pattern,\n",
    "                      \"stop_words\": stop_words}\n",
    "        if stemming:\n",
    "            \n",
    "            analyzer = CountVectorizer(**parameters).build_analyzer()\n",
    "            stemmer = nltk.stem.RSLPStemmer()\n",
    "            \n",
    "            def apply_stem(sentence):\n",
    "                return (stemmer.stem(w) for w in analyzer(sentence))\n",
    "        \n",
    "            parameters['analyzer'] = apply_stem\n",
    "        \n",
    "        super().__init__(**parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-miniature",
   "metadata": {},
   "source": [
    "### Pre-processor <a class=\"anchor\" id=\"pre-processor\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "monthly-south",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "text_transformer = Pipeline(steps=[\n",
    "    (\"text_column_concatenation\", TextColumnConcatenation()),\n",
    "    (\"count_vectorizer\", CountVectorizerWithStemming()),\n",
    "    (\"tf_idf\", \"passthrough\"),\n",
    "    (\"scaler\", StandardScaler(with_mean=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('numerical', numerical_transformer, numerical_columns),\n",
    "    ('categorical', text_transformer, text_columns)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-third",
   "metadata": {},
   "source": [
    "### Data Split <a class=\"anchor\" id=\"data-split\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-poverty",
   "metadata": {},
   "source": [
    "We divided the data into training and test subsets, using stratification so that the proportion of classes is maintained in both subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prompt-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed_value, test_size=.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-screen",
   "metadata": {},
   "source": [
    "## Modeling <a class=\"anchor\" id=\"modeling\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-registration",
   "metadata": {},
   "source": [
    "We used a combination of simple classifiers (Logistic Regression, Decision Tree) and more complex ones (Random Forest, Multi-Layer Perceptron). And we chose the best one based on the validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "atmospheric-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_log = Pipeline(\n",
    "    [\n",
    "        ('preprocessor', preprocessor),\n",
    "        (\"classifier\", LogisticRegression(random_state=seed_value))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_tree = Pipeline(\n",
    "    [\n",
    "        ('preprocessor', preprocessor),\n",
    "        (\"classifier\", DecisionTreeClassifier(random_state=seed_value))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_rf = Pipeline(\n",
    "    [\n",
    "        ('preprocessor', preprocessor),\n",
    "        (\"classifier\", RandomForestClassifier(random_state=seed_value))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_mlp = Pipeline(\n",
    "    [\n",
    "        ('preprocessor', preprocessor),\n",
    "        (\"classifier\", MLPClassifier(max_iter=100, random_state=seed_value))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-harvey",
   "metadata": {},
   "source": [
    "## Model Validation <a class=\"anchor\" id=\"model-validation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-trust",
   "metadata": {},
   "source": [
    "For validation and hyperparameter search, we used RandomSearch and StratifiedKfold. RandomSearch is usually better than GridSearch and less computationally expensive. \n",
    "\n",
    "Since we have an imbalanced dataset, we shouldn't use accuracy as our only metric. Therefore, we used f1, precision, recall and the ROC AUC score. Our random search optimization uses the ROC AUC score to return the best model.\n",
    "\n",
    "We also manually tweaked the hyperparameters for some promissing models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-jacket",
   "metadata": {},
   "source": [
    "### Random Search <a class=\"anchor\" id=\"random-search\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "indonesian-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dangerous-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'f1': 'f1_weighted', 'precision': 'precision_weighted', 'recall': 'recall_weighted', \n",
    "           'roc_auc': 'roc_auc_ovo_weighted'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "upper-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    with open(path, \"wb\") as file:\n",
    "        cp.dump(model, file)\n",
    "    print(\"Model saved at:\", path)\n",
    "\n",
    "    \n",
    "def load_model(path):\n",
    "    with open(path, \"rb\") as file:\n",
    "        return cp.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "photographic-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_pipeline(pipeline, search_space, scoring, cv, random_state, X, y, load_best=False, n_iter=10,\n",
    "                    model_name=None, save=False):\n",
    "    \"\"\" Creates a random search for pipeline, fits the data, and returns the random search object\n",
    "        \n",
    "        if load_best is true, then it skips training and loads the best model instead\n",
    "    \"\"\"\n",
    "    if model_name == None:\n",
    "        model_name = f\"RandomSearch{pipeline['classifier'].__class__.__name__}.pkl\"\n",
    "    \n",
    "    best_models_path = os.getenv(\"BEST_MODELS_PATH\")\n",
    "    model_path = os.path.join(best_models_path, model_name)\n",
    "    \n",
    "    if load_best:\n",
    "        print(\"Loading...\")\n",
    "        search = load_model(model_path)\n",
    "    else:\n",
    "        print(\"Training...\")\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_distributions=search_space,\n",
    "            cv=cv,\n",
    "            verbose=1,\n",
    "            n_jobs=-1,\n",
    "            random_state=random_state,\n",
    "            scoring=scoring,\n",
    "            refit=\"roc_auc\",\n",
    "            n_iter=n_iter,\n",
    "            return_train_score=True,\n",
    "        )\n",
    "        start = time.time()\n",
    "        search.fit(X_train, y_train)\n",
    "        stop = time.time()\n",
    "        print(f\"Training time: {(stop - start) / 60} minutes\")\n",
    "        if save:\n",
    "            save_model(search, model_path)       \n",
    "    \n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "brief-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_metrics(search):\n",
    "    best_index = search.best_index_\n",
    "    metrics = search.scorer_.keys()\n",
    "    data_modes = [\"train\", \"test\"]\n",
    "    statistics = [\"mean\", \"std\"]\n",
    "    \n",
    "    iterables = [data_modes, statistics]\n",
    "    index = pd.MultiIndex.from_product(iterables, names=[\"data\", \"statistic\"])\n",
    "    \n",
    "    df = pd.DataFrame(index=index, columns=metrics)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        for data_mode in data_modes:\n",
    "            for statistic in statistics:\n",
    "                df.loc[(data_mode, statistic), \n",
    "                       metric] =  search.cv_results_[f'{statistic}_{data_mode}_{metric}'][best_index]\n",
    "    return df.loc[:, :].astype(float).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-appeal",
   "metadata": {},
   "source": [
    "#### Text-Hyperparameters <a class=\"anchor\" id=\"text-hyperparameters\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-mississippi",
   "metadata": {},
   "source": [
    "We used hyperparameters in our data processing phase to help with generalization. These hyperparameters are \n",
    "\n",
    "* stemming\n",
    "* minimum and maximum word length \n",
    "* stop words. \n",
    "\n",
    "Very small or large words don't help our classifier as they don't contain meaningful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "golden-catch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['title'],\n",
       " ['query'],\n",
       " ['concatenated_tags'],\n",
       " ['title', 'query'],\n",
       " ['title', 'concatenated_tags'],\n",
       " ['query', 'concatenated_tags'],\n",
       " ['title', 'query', 'concatenated_tags']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_combinations = []\n",
    "for r in range(1, len(text_columns) + 1):\n",
    "    combination = []\n",
    "    for value in combinations(text_columns, r):\n",
    "        combination.append(list(value))\n",
    "    columns_combinations += combination\n",
    "columns_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "olympic-healthcare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'acerca',\n",
       " 'adeus',\n",
       " 'agora',\n",
       " 'ainda',\n",
       " 'alem',\n",
       " 'algmas',\n",
       " 'algo',\n",
       " 'algumas',\n",
       " 'alguns']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_url = \"https://raw.githubusercontent.com/stopwords-iso/stopwords-pt/master/stopwords-pt.txt\"\n",
    "stopwords = pd.read_csv(stopwords_url, header=None)[0].tolist()\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "three-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_hyperparams = {\n",
    "    \"preprocessor__categorical__text_column_concatenation__columns\": columns_combinations,\n",
    "    \"preprocessor__categorical__count_vectorizer__stop_words\": [None, stopwords],\n",
    "    \"preprocessor__categorical__count_vectorizer__min_word_length\": [3, 4, 5, 6],\n",
    "    \"preprocessor__categorical__count_vectorizer__max_word_length\": [15, 15, 16],\n",
    "    \"preprocessor__categorical__count_vectorizer__stemming\": [False, True],\n",
    "    \"preprocessor__categorical__tf_idf\": [\"passthrough\", TfidfTransformer()],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-language",
   "metadata": {},
   "source": [
    "#### Validation Metrics <a class=\"anchor\" id=\"validation-metrics\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "broad-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_best = False # set this flag to True if you want to run the random search\n",
    "n_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "weighted-bolivia",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3858e56511b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msearch_space_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_hyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m best_pipe_tree = search_pipeline(pipe_tree, search_space_tree, scoring, cv, seed_value, \n\u001b[0m\u001b[1;32m      9\u001b[0m                                  X_train, y_train, load_best=load_best, n_iter=n_iter, save=True)\n\u001b[1;32m     10\u001b[0m \u001b[0mget_search_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_pipe_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-b6e4b24e1e74>\u001b[0m in \u001b[0;36msearch_pipeline\u001b[0;34m(pipeline, search_space, scoring, cv, random_state, X, y, load_best, n_iter, model_name, save)\u001b[0m\n\u001b[1;32m     29\u001b[0m         )\n\u001b[1;32m     30\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training time: {(stop - start) / 60} minutes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1618\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1619\u001b[0;31m         evaluate_candidates(ParameterSampler(\n\u001b[0m\u001b[1;32m   1620\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m             random_state=self.random_state))\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "search_space_tree = {\n",
    "    \"classifier__criterion\": ['gini', 'entropy'],\n",
    "    \"classifier__max_depth\": [2, 4, 6, 8, 10, 12]\n",
    "}\n",
    "\n",
    "search_space_tree.update(text_hyperparams)\n",
    "\n",
    "best_pipe_tree = search_pipeline(pipe_tree, search_space_tree, scoring, cv, seed_value, \n",
    "                                 X_train, y_train, load_best=load_best, n_iter=n_iter, save=True)\n",
    "get_search_metrics(best_pipe_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-yemen",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space_log = {\n",
    "    \"classifier__penalty\": ['l2', 'l1'],\n",
    "    \"classifier__C\": np.logspace(0, 4, 10),\n",
    "    \"classifier__solver\":['newton-cg', 'saga', 'liblinear']\n",
    "}\n",
    "\n",
    "search_space_log.update(text_hyperparams)\n",
    "\n",
    "best_pipe_log = search_pipeline(pipe_log, search_space_log, scoring,  cv, seed_value,\n",
    "                                X_train, y_train, load_best=load_best, n_iter=n_iter, save=True);\n",
    "get_search_metrics(best_pipe_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space_rf = {\n",
    "    \"classifier__n_estimators\": [10, 100, 1000],\n",
    "    \"classifier__max_depth\":[5, 8, 15, 25, 30, None],\n",
    "    \"classifier__min_samples_leaf\":[1, 2, 5, 10, 15, 100],\n",
    "    \"classifier__max_leaf_nodes\": [2, 5, 10]\n",
    "}\n",
    "\n",
    "best_pipe_rf = search_pipeline(pipe_rf, search_space_rf, scoring, cv, seed_value, \n",
    "                               X_train, y_train, load_best=load_best, n_iter=n_iter, save=True)\n",
    "get_search_metrics(best_pipe_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf_text = Pipeline(\n",
    "    [\n",
    "        ('preprocessor', text_transformer),\n",
    "        (\"classifier\", RandomForestClassifier(random_state=seed_value, n_jobs=-1))\n",
    "    ]\n",
    ")\n",
    "\n",
    "params_rf_text = {\n",
    "    \"classifier__n_estimators\": [100],\n",
    "    \"preprocessor__text_column_concatenation__columns\": [['title', 'concatenated_tags']],\n",
    "    \"preprocessor__count_vectorizer__stop_words\": [stopwords],\n",
    "    \"preprocessor__count_vectorizer__max_word_length\": [15],\n",
    "    \"preprocessor__count_vectorizer__min_word_length\": [3],\n",
    "    \"preprocessor__count_vectorizer__stemming\": [False],\n",
    "    \"preprocessor__tf_idf\": [TfidfTransformer()],\n",
    "}\n",
    "\n",
    "manual_pipe_rf = search_pipeline(pipe_rf_text, params_rf_text, scoring, cv, seed_value, \n",
    "                                 X_train, y_train, load_best=False, n_iter=1, \n",
    "                                 model_name=\"ManualRandomForest.pkl\")\n",
    "\n",
    "get_search_metrics(manual_pipe_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space_mlp = {\n",
    "    'classifier__hidden_layer_sizes': [(10, 30, 10),(20,)],\n",
    "    'classifier__activation': ['tanh', 'relu'],\n",
    "    'classifier__solver': ['sgd', 'adam'],\n",
    "    'classifier__alpha': [0.0001, 0.05],\n",
    "    'classifier__learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "search_space_mlp.update(text_hyperparams)\n",
    "\n",
    "best_pipe_mlp = search_pipeline(pipe_mlp, search_space_mlp, scoring, cv, seed_value, \n",
    "                                X_train, y_train, load_best=load_best, n_iter=n_iter, save=True)\n",
    "\n",
    "get_search_metrics(best_pipe_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_list = [best_pipe_tree, best_pipe_log, best_pipe_rf, best_pipe_mlp, manual_pipe_rf]\n",
    "\n",
    "best_search_index = max(range(len(search_list)), key=lambda i: search_list[i].best_score_)\n",
    "best_search = search_list[best_search_index]\n",
    "\n",
    "best_score = best_search.best_score_\n",
    "best_model = best_search.best_estimator_\n",
    "\n",
    "print(f\"Best Model: {best_model['classifier'].__class__.__name__} | Best ROC-AUC: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_metrics_df = get_search_metrics(best_search)\n",
    "with open(os.getenv(\"METRICS_PATH\"), 'w') as f:\n",
    "    f.write(f\"Validation Metrics: \\n{validation_metrics_df.to_string()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-infrared",
   "metadata": {},
   "source": [
    "### Test Metrics  <a class=\"anchor\" id=\"test-metrics\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = best_model.predict(X_train)\n",
    "y_train_proba = best_model.predict_proba(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "y_test_proba = best_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_score_multiclass(y_true, y_proba, labels):\n",
    "\n",
    "    result = []\n",
    "    for label_index, current_label in enumerate(labels):\n",
    "\n",
    "        y_true_binary = (y_true == current_label).astype(float)\n",
    "        y_proba_correct = y_proba[:, label_index].reshape(-1,1)\n",
    "        \n",
    "        result.append(roc_auc_score(y_true_binary, y_proba_correct))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = best_model.classes_\n",
    "\n",
    "metrics = [\"f1\", \"precision\", \"recall\", \"roc_auc\"]\n",
    "data_mode_list = [\"train\", \"test\"]\n",
    "iterables = [metrics, data_mode_list]\n",
    "index = pd.MultiIndex.from_product(iterables, names=[\"metric\", \"data\"])\n",
    "\n",
    "test_metrics_df = pd.DataFrame(index=index, columns=labels)\n",
    "\n",
    "y_tuples = [(y_train, y_train_pred, y_train_proba), (y_test, y_test_pred, y_test_proba)]\n",
    "\n",
    "for data_mode, y_tuple in zip(data_mode_list, y_tuples):\n",
    "    y_true, y_pred, y_proba = y_tuple\n",
    "    test_metrics_df.loc[(\"f1\", data_mode), :] = f1_score(y_true, y_pred, average=None, labels=labels)\n",
    "    test_metrics_df.loc[(\"precision\", data_mode), :] = precision_score(y_true, y_pred, average=None, labels=labels)\n",
    "    test_metrics_df.loc[(\"recall\", data_mode), :] = recall_score(y_true, y_pred, average=None, labels=labels)\n",
    "    test_metrics_df.loc[(\"roc_auc\", data_mode), :] = roc_auc_score_multiclass(y_true, y_proba, labels=labels)\n",
    "\n",
    "test_metrics_df = test_metrics_df.astype(float).round(3)\n",
    "test_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getenv(\"METRICS_PATH\"), 'a') as f:\n",
    "    f.write(f\"\\n\\nTest Metrics: \\n{test_metrics_df.to_string()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "disp = plot_confusion_matrix(best_model, X_train, y_train, normalize='true', cmap=plt.cm.Blues, ax=ax, \n",
    "                      xticks_rotation=45, values_format=\".3f\")\n",
    "disp.ax_.set_title(\"Training Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "disp = plot_confusion_matrix(best_model, X_test, y_test, normalize='true', cmap=plt.cm.Blues, ax=ax, \n",
    "                      xticks_rotation=45, values_format=\".3f\")\n",
    "disp.ax_.set_title(\"Testing Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-selection",
   "metadata": {},
   "source": [
    "In addition to the metrics and the confusion matrix, we think it is important to have an idea of the distribution of the categories when using the model. The table below shows the percentage of each true class in the predicted ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "df = pd.DataFrame({\"True\": y_test, \"Predicted\": y_pred})\n",
    "print(\"Proportion of true classes in the predicted ones\")\n",
    "df.groupby([\"Predicted\"])['True'].value_counts(normalize=True).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "best_model.predict(X_test.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-dairy",
   "metadata": {},
   "source": [
    "## Model Exportation <a class=\"anchor\" id=\"model-exportation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(X, y); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(best_model, os.getenv(\"MODEL_PATH\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
